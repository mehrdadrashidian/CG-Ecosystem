{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOroTV5kzpKI61aYlG32qGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrdadrashidian/CG-Ecosystem/blob/master/Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZVyEbb387v3"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import librosa\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "from pymilvus import connections, Collection, utility\n",
        "import decord\n",
        "from transformers import TimesformerModel, TimesformerConfig\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Initialize BERT model for text\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize ResNet model for images\n",
        "image_model = models.resnet50(pretrained=True)\n",
        "image_model.eval()\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Initialize TimeSformer model for video\n",
        "video_model_config = TimesformerConfig.from_pretrained('facebook/timesformer-base-finetuned-k400')\n",
        "video_model = TimesformerModel.from_pretrained('facebook/timesformer-base-finetuned-k400')\n",
        "\n",
        "# Milvus configuration\n",
        "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
        "collection_name = \"asset_vectors\"\n",
        "if not utility.has_collection(collection_name):\n",
        "    collection = Collection(collection_name, {\n",
        "        \"fields\": [\n",
        "            {\"name\": \"asset_id\", \"type\": \"VARCHAR\", \"max_length\": 255},\n",
        "            {\"name\": \"vector\", \"type\": \"FLOAT_VECTOR\", \"dim\": 768},\n",
        "            {\"name\": \"asset_type\", \"type\": \"VARCHAR\", \"max_length\": 50},\n",
        "            {\"name\": \"tags\", \"type\": \"VARCHAR\", \"max_length\": 255}\n",
        "        ],\n",
        "        \"primary_field\": \"asset_id\"\n",
        "    })\n",
        "    collection.create_index(field_name=\"vector\", index_params={\"metric_type\": \"IP\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}})\n",
        "else:\n",
        "    collection = Collection(collection_name)\n",
        "\n",
        "# Function to extract features based on Asset Type\n",
        "def extract_features(asset_type, file_path):\n",
        "    if asset_type == \"text\":\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        return text_model.encode(text)\n",
        "    elif asset_type == \"audio\":\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "        return np.mean(mfcc.T, axis=0)\n",
        "    elif asset_type == \"image\":\n",
        "        img = Image.open(file_path).convert('RGB')\n",
        "        img = image_transform(img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            features = image_model(img).squeeze(0).numpy()\n",
        "        return features\n",
        "    elif asset_type == \"video\":\n",
        "        vr = decord.VideoReader(file_path)\n",
        "        frames = [vr[i].asnumpy() for i in range(0, len(vr), len(vr)//8)]  # Sample 8 frames\n",
        "        processed_frames = torch.stack([\n",
        "            image_transform(Image.fromarray(frame)) for frame in frames\n",
        "        ]).unsqueeze(0)  # Batch size of 1\n",
        "        with torch.no_grad():\n",
        "            features = video_model(pixel_values=processed_frames).last_hidden_state.mean(dim=1).squeeze(0).numpy()\n",
        "        return features\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported asset type\")\n",
        "\n",
        "# Normalize and store vectors in Milvus\n",
        "def store_asset(asset_id, asset_type, file_path, tags):\n",
        "    vector = extract_features(asset_type, file_path)\n",
        "    collection.insert([[asset_id, vector.tolist(), asset_type, \",\".join(tags)]])\n",
        "\n",
        "# Endpoint to upload an asset\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_asset():\n",
        "    try:\n",
        "        data = request.json\n",
        "        asset_id = data['asset_id']\n",
        "        asset_type = data['asset_type']\n",
        "        file_path = data['file_path']\n",
        "        tags = data.get('tags', [])\n",
        "\n",
        "        if asset_type not in ['text', 'audio', 'image', 'video']:\n",
        "            return jsonify({\"error\": \"Invalid asset type\"}), 400\n",
        "\n",
        "        store_asset(asset_id, asset_type, file_path, tags)\n",
        "\n",
        "        return jsonify({\"message\": \"Asset uploaded successfully\"}), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Endpoint to get recommendations based on an asset\n",
        "@app.route('/recommend', methods=['POST'])\n",
        "def recommend():\n",
        "    try:\n",
        "        data = request.json\n",
        "        asset_id = data['asset_id']\n",
        "        top_n = data.get('top_n', 5)\n",
        "\n",
        "        query = collection.query(expr=f\"asset_id == '{asset_id}'\", output_fields=[\"vector\"])\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Asset ID not found\"}), 404\n",
        "\n",
        "        query_vector = np.array(query[0]['vector']).reshape(1, -1)\n",
        "        search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}\n",
        "        results = collection.search(query_vector.tolist(), \"vector\", search_params, limit=top_n, output_fields=[\"asset_id\", \"asset_type\"])\n",
        "\n",
        "        recommendations = [\n",
        "            {\"asset_id\": hit.entity.get(\"asset_id\"), \"similarity\": hit.distance}\n",
        "            for hit in results\n",
        "        ]\n",
        "\n",
        "        return jsonify({\"recommendations\": recommendations}), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Endpoint to list all assets\n",
        "@app.route('/assets', methods=['GET'])\n",
        "def list_assets():\n",
        "    try:\n",
        "        assets = collection.query(expr=\"\", output_fields=[\"asset_id\", \"asset_type\", \"tags\"])\n",
        "        return jsonify(assets), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    }
  ]
}