{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrdadrashidian/CG-Ecosystem/blob/master/Media_Process_Unit_with_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqTw8AZd8379",
        "outputId": "5b5786f6-654b-4317-f329-1db456a10d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.11/dist-packages (2.9.10)\n",
            "Requirement already satisfied: Flask-SQLAlchemy in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: flask>=2.2.5 in /usr/local/lib/python3.11/dist-packages (from Flask-SQLAlchemy) (3.1.0)\n",
            "Requirement already satisfied: sqlalchemy>=2.0.16 in /usr/local/lib/python3.11/dist-packages (from Flask-SQLAlchemy) (2.0.37)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.2.5->Flask-SQLAlchemy) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.2.5->Flask-SQLAlchemy) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.2.5->Flask-SQLAlchemy) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.2.5->Flask-SQLAlchemy) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=2.2.5->Flask-SQLAlchemy) (1.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.16->Flask-SQLAlchemy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.16->Flask-SQLAlchemy) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=2.2.5->Flask-SQLAlchemy) (3.0.2)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.11/dist-packages (5.2.1)\n",
            "Requirement already satisfied: Flask-Redis in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from Flask-Redis) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->Flask-Redis) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->Flask-Redis) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->Flask-Redis) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->Flask-Redis) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->Flask-Redis) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->Flask-Redis) (3.0.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.11/dist-packages (4.3.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash) (11.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.13.1)\n",
            "Requirement already satisfied: nsfw-detector in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from nsfw-detector) (11.1.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.7.0 in /usr/local/lib/python3.11/dist-packages (from nsfw-detector) (0.7.0)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from nsfw-detector) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.7.0->nsfw-detector) (1.26.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.7.0->nsfw-detector) (1.17.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.7.0->nsfw-detector) (4.25.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->nsfw-detector) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->nsfw-detector) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.1.0->nsfw-detector) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.1.0->nsfw-detector) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.1.0->nsfw-detector) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->nsfw-detector) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->nsfw-detector) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->nsfw-detector) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->nsfw-detector) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.1.0->nsfw-detector) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.1.0->nsfw-detector) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.1.0->nsfw-detector) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.1.0->nsfw-detector) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.1.0->nsfw-detector) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.1.0->nsfw-detector) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.1.0->nsfw-detector) (0.1.2)\n",
            "Requirement already satisfied: piexif in /usr/local/lib/python3.11/dist-packages (1.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install Flask\n",
        "!pip install faiss-cpu\n",
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "!pip install psycopg2-binary Flask-SQLAlchemy\n",
        "!pip install redis Flask-Redis\n",
        "!pip install pyngrok\n",
        "!pip install pillow\n",
        "!pip install imagehash\n",
        "!pip install nsfw-detector\n",
        "!pip install piexif\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n222hxZK9n8P",
        "outputId": "ea157e26-b0d5-4570-8d9d-0f5b37c37a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /mobilenet_v2_140_224.1.zip\n",
            "replace nsfw_model/mobilenet_v2_140_224/class_labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y A\n",
            " extracting: nsfw_model/mobilenet_v2_140_224/class_labels.txt  \n",
            "replace nsfw_model/mobilenet_v2_140_224/frozen_graph.pb? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/frozen_graph.pb  \n",
            "replace nsfw_model/mobilenet_v2_140_224/saved_model.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/saved_model.h5  \n",
            "replace nsfw_model/mobilenet_v2_140_224/saved_model.pb? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/saved_model.pb  \n",
            "replace nsfw_model/mobilenet_v2_140_224/saved_model.tflite? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/saved_model.tflite  \n",
            "replace nsfw_model/mobilenet_v2_140_224/saved_model_weights.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/saved_model_weights.h5  \n",
            "replace nsfw_model/mobilenet_v2_140_224/training_results.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/training_results.txt  \n",
            "replace nsfw_model/mobilenet_v2_140_224/variables/variables.data-00000-of-00002? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/variables/variables.data-00000-of-00002  \n",
            "replace nsfw_model/mobilenet_v2_140_224/variables/variables.data-00001-of-00002? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/variables/variables.data-00001-of-00002  \n",
            "replace nsfw_model/mobilenet_v2_140_224/variables/variables.index? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/variables/variables.index  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model/group1-shard1of5.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model/group1-shard1of5.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model/group1-shard2of5.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model/group1-shard2of5.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model/group1-shard3of5.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model/group1-shard3of5.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model/group1-shard4of5.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model/group1-shard4of5.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model/group1-shard5of5.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model/group1-shard5of5.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model/model.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model/model.json  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model_quantized/group1-shard1of2.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model_quantized/group1-shard1of2.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model_quantized/group1-shard2of2.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model_quantized/group1-shard2of2.bin  \n",
            "replace nsfw_model/mobilenet_v2_140_224/web_model_quantized/model.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: nsfw_model/mobilenet_v2_140_224/web_model_quantized/model.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip /mobilenet_v2_140_224.1.zip -d nsfw_model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_favr6vXzui",
        "outputId": "d8556d15-471a-4cb9-8024-07be9dc9fc1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ŸÖÿØŸÑ NSFW ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ¥ÿØ!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_path = \"/content/nsfw_model/mobilenet_v2_140_224\"\n",
        "nsfw_model = tf.saved_model.load(model_path)\n",
        "\n",
        "print(\"‚úÖ ŸÖÿØŸÑ NSFW ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ¥ÿØ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAsQHHwaUstC",
        "outputId": "787ad4ce-f98c-40e4-ce88-2550ebae205f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ ŸÖÿ≠ÿ™Ÿà€åÿßÿ™ ŸæŸàÿ¥Ÿá ŸÖÿØŸÑ: ['saved_model.tflite', 'frozen_graph.pb', 'saved_model_weights.h5', 'saved_model.pb', 'assets', 'variables', 'web_model_quantized', 'training_results.txt', 'web_model', 'class_labels.txt', 'saved_model.h5']\n",
            "üìÇ ŸÖÿ≠ÿ™Ÿà€åÿßÿ™ web_model_quantized: ['group1-shard2of2.bin', 'model.json', 'group1-shard1of2.bin']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model_root = \"/content/nsfw_model/mobilenet_v2_140_224\"\n",
        "print(\"üìÇ ŸÖÿ≠ÿ™Ÿà€åÿßÿ™ ŸæŸàÿ¥Ÿá ŸÖÿØŸÑ:\", os.listdir(model_root))\n",
        "\n",
        "web_model_path = os.path.join(model_root, \"web_model_quantized\")\n",
        "if os.path.exists(web_model_path):\n",
        "    print(\"üìÇ ŸÖÿ≠ÿ™Ÿà€åÿßÿ™ web_model_quantized:\", os.listdir(web_model_path))\n",
        "else:\n",
        "    print(\"‚ùå ŸæŸàÿ¥Ÿá web_model_quantized Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ±ÿØ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G_QQWu_9osE",
        "outputId": "becadba8-f9ed-4a5b-a6d6-b4df5dfb23a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ŸÅÿß€åŸÑ ŸÖÿØŸÑ Ÿæ€åÿØÿß ÿ¥ÿØ!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model_path = \"/content/nsfw_model/mobilenet_v2_140_224/saved_model.h5\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"‚úÖ ŸÅÿß€åŸÑ ŸÖÿØŸÑ Ÿæ€åÿØÿß ÿ¥ÿØ!\")\n",
        "else:\n",
        "    print(\"‚ùå ŸÅÿß€åŸÑ ŸÖÿØŸÑ ÿØÿ± ŸÖÿ≥€åÿ± ŸÖÿ¥ÿÆÿµ‚Äåÿ¥ÿØŸá Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ±ÿØ.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8bJD_Xl-Yec",
        "outputId": "3ed195e2-5b0e-465f-db8c-30471f1455b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tensorflow-hub==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.7.0) (1.26.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.7.0) (1.17.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.7.0) (4.25.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install tensorflow-hub==0.7.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-312gxcQ-j9M",
        "outputId": "9a1fe099-6abe-4dc5-db71-2fdd57f77a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/162.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.9/162.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "7035cf397dec42058b0872e123b343a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.14.0\n",
            "  Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.70.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.0)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "y\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "4ccb9671886746eb844a7c109de4a9d2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install protobuf==3.20.*\n",
        "!pip install tensorflow==2.14.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jazZeG2Mtb0O",
        "outputId": "dced0517-ca5e-4e5c-8fec-b6bc56a822a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "postgresql-contrib is already the newest version (14+238).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install postgresql postgresql-contrib\n",
        "!service postgresql start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1oKyW0ttu4n",
        "outputId": "5305ec23-5abb-4c75-cdab-191e68e80487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  role \"myuser\" already exists\n",
            "ERROR:  database \"mydb\" already exists\n"
          ]
        }
      ],
      "source": [
        "!sudo -u postgres psql -c \"CREATE USER myuser WITH PASSWORD 'mypassword';\"\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE mydb OWNER myuser;\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VZqtoLx8mte",
        "outputId": "4fd50411-f339-4947-d6e9-a081ed8466e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:From <ipython-input-1-0fe8ec8eef34>:86: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.saved_model.load` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç ÿßÿ¨ÿ±ÿß ÿ¥ÿØŸá ÿØÿ±: NgrokTunnel: \"https://fada-34-80-110-253.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:38:53] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:38:53] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:38:54] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:38:58] \"POST /calculate_nudity HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:00] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:03] \"POST /search HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:04] \"GET /static/uploads/pan.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:04] \"GET /static/uploads/56ee384b7ada0e960c9576c0e883a8c2_low.webp HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:04] \"GET /static/uploads/56b72279-9d2d-41f1-894a-3fbbea9cb967.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:04] \"GET /static/uploads/photo_2025-02-05_19-21-58.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:04] \"GET /static/uploads/1-intro-photo-final.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Feb/2025 18:39:04] \"GET /static/uploads/masha.png HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!pkill -9 ngrok\n",
        "from flask import Flask, request, jsonify, render_template_string, send_from_directory\n",
        "from pyngrok import ngrok\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import tensorflow.compat.v1 as tf\n",
        "import timm\n",
        "from sqlalchemy import create_engine, Column, Integer, String, LargeBinary, text\n",
        "from sqlalchemy.orm import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "import logging\n",
        "import uuid  # ÿ®ÿ±ÿß€å ÿ™ŸàŸÑ€åÿØ ŸÜÿßŸÖ €å⁄©ÿ™ÿß\n",
        "import os\n",
        "import piexif\n",
        "from PIL import Image, PngImagePlugin\n",
        "import logging\n",
        "import datetime\n",
        "import binascii\n",
        "\n",
        "# ÿ™ŸÜÿ∏€åŸÖ logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# ÿ™ŸÜÿ∏€åŸÖÿßÿ™ Ÿæÿß€å⁄ØÿßŸá ÿØÿßÿØŸá PostgreSQL\n",
        "DATABASE_URL = \"postgresql://myuser:mypassword@localhost/mydb\"\n",
        "Base = declarative_base()\n",
        "\n",
        "# ŸÖÿØŸÑ ÿØ€åÿ™ÿßÿ®€åÿ≥\n",
        "class ImageEmbedding(Base):\n",
        "    __tablename__ = 'image_embeddings'\n",
        "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
        "    filename = Column(String, unique=True, nullable=False)\n",
        "    image_data = Column(LargeBinary, nullable=False)  # ÿ∞ÿÆ€åÿ±Ÿá ÿ™ÿµŸà€åÿ± ÿ®Ÿá‚ÄåÿµŸàÿ±ÿ™ ÿ®ÿß€åŸÜÿ±€å\n",
        "    embedding = Column(LargeBinary, nullable=False)   # ÿ∞ÿÆ€åÿ±Ÿá ÿ®ÿ±ÿØÿßÿ± Ÿà€å⁄ò⁄Ø€å ÿ®Ÿá‚ÄåÿµŸàÿ±ÿ™ ÿ®ÿß€åÿ™\n",
        "    nudity_score = Column(String, nullable=True)  # ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ ŸÜŸÖÿ±Ÿá ŸÖÿ≠ÿ™Ÿàÿß€å ŸÜÿßŸÖŸÜÿßÿ≥ÿ®\n",
        "\n",
        "engine = create_engine(DATABASE_URL)\n",
        "SessionLocal = sessionmaker(bind=engine)\n",
        "Base.metadata.create_all(engine)\n",
        "\n",
        "# ÿ®ÿ±ÿ±ÿ≥€å Ÿà ÿßŸÅÿ≤ŸàÿØŸÜ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß€å ŸÖŸÅŸÇŸàÿØ ÿ®Ÿá ÿØ€åÿ™ÿßÿ®€åÿ≥\n",
        "def check_and_add_columns():\n",
        "    session = SessionLocal()\n",
        "    try:\n",
        "        result = session.execute(text(\"SELECT column_name FROM information_schema.columns WHERE table_name='image_embeddings' AND column_name='nudity_score'\"))\n",
        "        if not result.fetchone():\n",
        "            session.execute(text(\"ALTER TABLE image_embeddings ADD COLUMN nudity_score VARCHAR;\"))\n",
        "            session.commit()\n",
        "            logging.info(\"‚úÖ ÿ≥ÿ™ŸàŸÜ nudity_score ÿ®Ÿá ÿØ€åÿ™ÿßÿ®€åÿ≥ ÿßÿ∂ÿßŸÅŸá ÿ¥ÿØ.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿ®ÿ±ÿ±ÿ≥€å/ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß: {e}\")\n",
        "    finally:\n",
        "        session.close()\n",
        "\n",
        "check_and_add_columns()\n",
        "\n",
        "# ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿßŸàŸÑ€åŸá\n",
        "UPLOAD_FOLDER = 'static/uploads'\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "NGROK_TOKEN = \"2srSLcJso0gWDqNC7YWfGjNkHF0_5quu1vDqrrQtrjCFytuN3\"\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# ÿ™ŸÜÿ∏€åŸÖ FAISS\n",
        "D = 512\n",
        "index = faiss.IndexFlatL2(D)\n",
        "index_path = \"faiss.index\"\n",
        "if os.path.exists(index_path):\n",
        "    index = faiss.read_index(index_path)\n",
        "\n",
        "# ÿ™ŸÜÿ∏€åŸÖ ŸÖÿØŸÑ Ÿà€å⁄ò⁄Ø€å‚Äåÿ®ÿ±ÿØÿßÿ±€å\n",
        "feature_extractor = timm.create_model(\"tf_efficientnet_b0\", pretrained=True, num_classes=0)\n",
        "feature_extractor.eval()\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ÿ™ŸÜÿ∏€åŸÖ ŸÖÿØŸÑ NSFW\n",
        "tf.disable_eager_execution()\n",
        "session_tf = tf.Session()\n",
        "session_tf.run(tf.global_variables_initializer())\n",
        "model_path = \"/content/nsfw_model/mobilenet_v2_140_224\"\n",
        "with session_tf.as_default():\n",
        "    with session_tf.graph.as_default():\n",
        "        nsfw_model = tf.saved_model.loader.load(session_tf, [tf.saved_model.SERVING], model_path)\n",
        "        infer = nsfw_model.signature_def[\"serving_default\"]\n",
        "        input_tensor_name = infer.inputs[\"input\"].name\n",
        "        output_tensor_name = infer.outputs[\"prediction\"].name\n",
        "        input_tensor = session_tf.graph.get_tensor_by_name(input_tensor_name)\n",
        "        output_tensor = session_tf.graph.get_tensor_by_name(output_tensor_name)\n",
        "\n",
        "def detect_nudity(image_path):\n",
        "    with session_tf.as_default():\n",
        "        with session_tf.graph.as_default():\n",
        "            img = Image.open(image_path).convert(\"RGB\").resize((224, 224))\n",
        "            img_array = np.array(img).astype(np.float32) / 255.0\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "            result = session_tf.run(output_tensor, feed_dict={input_tensor: img_array})\n",
        "            return {\"sexy\": f\"{round(float(result[0][4]) * 100, 2)}%\", \"porn\": f\"{round(float(result[0][3]) * 100, 2)}%\"}\n",
        "\n",
        "def generate_image_embedding(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        img = np.array(img).astype('float32') / 255.0\n",
        "        img = torch.tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            embedding = feature_extractor(img).numpy()\n",
        "        embedding = embedding.reshape(1, -1)\n",
        "        if embedding.shape[1] > D:\n",
        "            embedding = embedding[:, :D]\n",
        "        elif embedding.shape[1] < D:\n",
        "            padding = np.zeros((1, D - embedding.shape[1]), dtype=embedding.dtype)\n",
        "            embedding = np.concatenate([embedding, padding], axis=1)\n",
        "        if not isinstance(embedding, np.ndarray):\n",
        "            logging.error(f\"‚ùå ÿÆÿ±Ÿàÿ¨€å embedding ŸÖŸÇÿØÿßÿ± ŸÜÿßŸÖÿπÿ™ÿ®ÿ±€å ÿØÿßÿ±ÿØ: {type(embedding)}\")\n",
        "            return None\n",
        "        logging.info(f\"‚úÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ embedding ŸÖŸàŸÅŸÇ€åÿ™‚Äåÿ¢ŸÖ€åÿ≤ ÿ®ŸàÿØ. ÿ¥⁄©ŸÑ ŸÜŸáÿß€å€å: {embedding.shape}\")\n",
        "        return embedding.astype('float32')\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "# ÿßÿµŸÑÿßÿ≠ ÿ™ÿßÿ®ÿπ ÿ®ÿ±ÿ±ÿ≥€å ÿ™ÿµŸà€åÿ± ÿ™⁄©ÿ±ÿßÿ±€å\n",
        "def check_duplicate_image(filename, embedding, sim_threshold=0.9):\n",
        "    session_db = SessionLocal()\n",
        "    try:\n",
        "        # ÿ®ÿ±ÿ±ÿ≥€å ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ŸÜÿßŸÖ ŸÅÿß€åŸÑ\n",
        "        duplicate_name = session_db.query(ImageEmbedding).filter_by(filename=filename).first()\n",
        "        if duplicate_name:\n",
        "            session_db.close()\n",
        "            return {\n",
        "                \"duplicate\": True,\n",
        "                \"reason\": \"filename\",\n",
        "                \"message\": f\"ÿπ⁄©ÿ≥ ÿØ€å⁄Øÿ±€å ÿ®ÿß ÿß€åŸÜ ŸÜÿßŸÖ Ÿà ŸÅÿ±ŸÖÿ™ Ÿæ€åÿ¥ÿ™ÿ± ÿ´ÿ®ÿ™ ÿ¥ÿØŸá ÿßÿ≥ÿ™: {filename}\"\n",
        "            }\n",
        "        # ÿ®ÿ±ÿ±ÿ≥€å ÿ¥ÿ®ÿßŸáÿ™ embedding ÿ®ÿ±ÿß€å ÿ™ŸÖÿßŸÖ ÿ™ÿµÿßŸà€åÿ± (ŸÜÿßŸÖ‚ÄåŸáÿß€å ŸÖÿ™ŸÅÿßŸàÿ™)\n",
        "        embeddings_db, filenames = load_embeddings_from_db()\n",
        "        if embeddings_db.shape[0] == 0:\n",
        "            session_db.close()\n",
        "            return {\"duplicate\": False}\n",
        "        normalized_query = embedding / np.linalg.norm(embedding)\n",
        "        norms = np.linalg.norm(embeddings_db, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1\n",
        "        normalized_embeddings = embeddings_db / norms\n",
        "        cosine_similarities = np.dot(normalized_query, normalized_embeddings.T)[0]\n",
        "        similar_indices = np.where(cosine_similarities >= sim_threshold)[0]\n",
        "        if similar_indices.size > 0:\n",
        "            similar_files = [filenames[i] for i in similar_indices]\n",
        "            session_db.close()\n",
        "            return {\n",
        "                \"duplicate\": True,\n",
        "                \"reason\": \"embedding\",\n",
        "                \"message\": f\"ÿ™ÿµŸà€åÿ± ÿ®ÿß ŸÖÿ¥ÿßÿ®Ÿáÿ™ ÿ®ÿßŸÑÿß€å 90 ÿØÿ±ÿµÿØ: {', '.join(similar_files)}\",\n",
        "                \"similar_files\": similar_files\n",
        "            }\n",
        "        session_db.close()\n",
        "        return {\"duplicate\": False}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿ®ÿ±ÿ±ÿ≥€å ÿ™ÿµŸà€åÿ± ÿ™⁄©ÿ±ÿßÿ±€å: {e}\")\n",
        "        session_db.close()\n",
        "        return {\"duplicate\": False}\n",
        "\n",
        "def save_to_database(filename, image_path, embedding, nudity_score):\n",
        "    try:\n",
        "        session_db = SessionLocal()\n",
        "        existing_entry = session_db.query(ImageEmbedding).filter_by(filename=filename).first()\n",
        "        if existing_entry:\n",
        "            logging.warning(f\"‚ö†Ô∏è ŸÅÿß€åŸÑ {filename} ÿßÿ≤ ŸÇÿ®ŸÑ ÿØÿ± ÿØ€åÿ™ÿßÿ®€åÿ≥ Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ.\")\n",
        "            session_db.close()\n",
        "            return False\n",
        "        embedding_bytes = embedding.tobytes()\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            image_data = img_file.read()\n",
        "        new_entry = ImageEmbedding(\n",
        "            filename=filename,\n",
        "            image_data=image_data,\n",
        "            embedding=embedding_bytes,\n",
        "            nudity_score=str(nudity_score)\n",
        "        )\n",
        "        session_db.add(new_entry)\n",
        "        session_db.commit()\n",
        "        logging.info(f\"‚úÖ ÿ™ÿµŸà€åÿ± {filename} ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿØÿ± ÿØ€åÿ™ÿßÿ®€åÿ≥ ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ.\")\n",
        "        session_db.close()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        session_db.rollback()\n",
        "        logging.error(f\"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿ∞ÿÆ€åÿ±Ÿá ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿØÿ± ÿØ€åÿ™ÿßÿ®€åÿ≥: {e}\")\n",
        "        session_db.close()\n",
        "        return False\n",
        "\n",
        "def load_embeddings_from_db():\n",
        "    session_db = SessionLocal()\n",
        "    images = session_db.query(ImageEmbedding).all()\n",
        "    session_db.close()\n",
        "    embeddings = [np.frombuffer(img.embedding, dtype=np.float32) for img in images]\n",
        "    filenames = [img.filename for img in images]\n",
        "    return np.vstack(embeddings), filenames\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return filename.lower().endswith(('png', 'jpg', 'jpeg', 'webp'))\n",
        "\n",
        "# --------------------------\n",
        "# ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ endpoint ÿ®ÿ±ÿß€å ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÜŸàÿØ€åÿ™€å (ÿ®ÿØŸàŸÜ ÿ´ÿ®ÿ™ ÿ™ÿµŸà€åÿ±)\n",
        "# --------------------------\n",
        "@app.route('/calculate_nudity', methods=['POST'])\n",
        "def calculate_nudity():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"ŸÅÿß€åŸÑ ÿßÿ±ÿ≥ÿßŸÑ ŸÜÿ¥ÿØŸá ÿßÿ≥ÿ™.\"})\n",
        "    file = request.files['file']\n",
        "    if file.filename == '' or not allowed_file(file.filename):\n",
        "        return jsonify({\"error\": \"ŸÜŸàÿπ ŸÅÿß€åŸÑ ŸÖÿπÿ™ÿ®ÿ± ŸÜ€åÿ≥ÿ™.\"})\n",
        "    temp_filename = \"temp_\" + uuid.uuid4().hex + \"_\" + file.filename\n",
        "    file_path = os.path.join(UPLOAD_FOLDER, temp_filename)\n",
        "    file.save(file_path)\n",
        "    try:\n",
        "        nudity_score = detect_nudity(file_path)\n",
        "        os.remove(file_path)\n",
        "        return jsonify({\"nudity_score\": nudity_score})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": \"ÿÆÿ∑ÿß ÿØÿ± ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÜŸàÿØ€åÿ™€å\", \"detail\": str(e)})\n",
        "\n",
        "# --------------------------\n",
        "# ÿµŸÅÿ≠Ÿá HTML ÿßÿµŸÑ€å ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Template\n",
        "# --------------------------\n",
        "html_page = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"fa\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>ŸÖÿØ€åÿ±€åÿ™ ÿ™ÿµÿßŸà€åÿ±</title>\n",
        "  <style>\n",
        "    /* ÿßÿ≥ÿ™ÿß€åŸÑ‚ÄåŸáÿß€å Ÿæÿß€åŸá */\n",
        "    body {\n",
        "      font-family: Arial, sans-serif;\n",
        "      background-color: #f2f2f2;\n",
        "      margin: 0;\n",
        "      padding: 0;\n",
        "      direction: rtl;\n",
        "      text-align: center;\n",
        "    }\n",
        "    .container {\n",
        "      max-width: 800px;\n",
        "      margin: 0 auto;\n",
        "      padding: 20px;\n",
        "    }\n",
        "    h1 {\n",
        "      color: #333;\n",
        "    }\n",
        "    /* ŸÜÿßÿ≠€åŸá ÿØÿ±⁄Ø‚ÄåÿßŸÜÿØ-ÿØÿ±ÿßŸæ */\n",
        "    .drop-zone {\n",
        "      border: 2px dashed #cccccc;\n",
        "      border-radius: 10px;\n",
        "      padding: 40px;\n",
        "      background-color: #fff;\n",
        "      cursor: pointer;\n",
        "      transition: border-color 0.3s ease;\n",
        "    }\n",
        "    .drop-zone.hover {\n",
        "      border-color: #555;\n",
        "    }\n",
        "    #preview {\n",
        "      max-width: 100%;\n",
        "      max-height: 300px;\n",
        "      margin-top: 20px;\n",
        "      display: none;\n",
        "    }\n",
        "    /* ÿßÿ≥ÿ™ÿß€åŸÑ ÿØ⁄©ŸÖŸá‚ÄåŸáÿß */\n",
        "    .btn {\n",
        "      display: inline-block;\n",
        "      margin: 10px;\n",
        "      padding: 10px 20px;\n",
        "      font-size: 16px;\n",
        "      color: #fff;\n",
        "      background-color: #007BFF;\n",
        "      border: none;\n",
        "      border-radius: 5px;\n",
        "      cursor: pointer;\n",
        "    }\n",
        "    .btn:hover {\n",
        "      background-color: #0056b3;\n",
        "    }\n",
        "    /* ÿßÿ≥ÿ™ÿß€åŸÑ ÿ™ÿßŸÖÿ®ŸÜ€åŸÑ‚ÄåŸáÿß€å ÿ™ÿµÿßŸà€åÿ± */\n",
        "    .thumbnail {\n",
        "      width: 150px;\n",
        "      height: 150px;\n",
        "      object-fit: cover;\n",
        "      border: 2px solid #ddd;\n",
        "      border-radius: 5px;\n",
        "      cursor: pointer;\n",
        "      transition: border-color 0.3s ease;\n",
        "    }\n",
        "    .thumbnail:hover {\n",
        "      border-color: #007BFF;\n",
        "    }\n",
        "    .thumbnails-container {\n",
        "      display: flex;\n",
        "      flex-wrap: wrap;\n",
        "      justify-content: center;\n",
        "      margin-top: 20px;\n",
        "    }\n",
        "    .thumb-container {\n",
        "      display: inline-block;\n",
        "      text-align: center;\n",
        "      margin: 10px;\n",
        "    }\n",
        "    /* ÿßÿ≥ÿ™ÿß€åŸÑ ŸÖÿØÿßŸÑ ŸÜŸÖÿß€åÿ¥ ÿ™ÿµŸà€åÿ± ⁄©ÿßŸÖŸÑ */\n",
        "    .modal {\n",
        "      display: none;\n",
        "      position: fixed;\n",
        "      z-index: 1000;\n",
        "      left: 0;\n",
        "      top: 0;\n",
        "      width: 100%;\n",
        "      height: 100%;\n",
        "      overflow: auto;\n",
        "      background-color: rgba(0,0,0,0.7);\n",
        "    }\n",
        "    .modal-content {\n",
        "      margin: 5% auto;\n",
        "      display: block;\n",
        "      max-width: 80%;\n",
        "    }\n",
        "    .close {\n",
        "      position: absolute;\n",
        "      top: 20px;\n",
        "      right: 35px;\n",
        "      color: #f1f1f1;\n",
        "      font-size: 40px;\n",
        "      font-weight: bold;\n",
        "      cursor: pointer;\n",
        "    }\n",
        "    /* Ÿæ€åÿßŸÖ‚ÄåŸáÿß */\n",
        "    #message {\n",
        "      margin-top: 20px;\n",
        "      font-size: 16px;\n",
        "    }\n",
        "    /* ŸÜŸÖÿß€åÿ¥ ŸÜŸàÿØ€åÿ™€å */\n",
        "    #nudity-score {\n",
        "      margin-top: 10px;\n",
        "      font-size: 18px;\n",
        "      font-weight: bold;\n",
        "      color: #444;\n",
        "    }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"container\">\n",
        "    <h1>ŸÖÿØ€åÿ±€åÿ™ ÿ™ÿµÿßŸà€åÿ±</h1>\n",
        "\n",
        "    <!-- ŸÜÿßÿ≠€åŸá ÿßŸÜÿ™ÿÆÿßÿ® ŸÅÿß€åŸÑ (ÿØÿ±⁄Ø‚ÄåÿßŸÜÿØ-ÿØÿ±ÿßŸæ €åÿß ⁄©ŸÑ€å⁄© ÿ®ÿ±ÿß€å ÿßŸÜÿ™ÿÆÿßÿ®) -->\n",
        "    <div id=\"drop-zone\" class=\"drop-zone\">\n",
        "      <p>ÿ®ÿ±ÿß€å ÿßŸÜÿ™ÿÆÿßÿ® ÿ™ÿµŸà€åÿ± ⁄©ŸÑ€å⁄© ⁄©ŸÜ€åÿØ €åÿß ÿ™ÿµŸà€åÿ± ÿ±ÿß ÿ®⁄©ÿ¥€åÿØ Ÿà ÿ±Ÿáÿß ⁄©ŸÜ€åÿØ</p>\n",
        "      <input type=\"file\" id=\"file-input\" accept=\"image/*\" style=\"display: none;\">\n",
        "      <img id=\"preview\" src=\"#\" alt=\"Ÿæ€åÿ¥‚ÄåŸÜŸÖÿß€åÿ¥ ÿ™ÿµŸà€åÿ±\">\n",
        "    </div>\n",
        "    <!-- ŸÜŸÖÿß€åÿ¥ ŸÜŸàÿØ€åÿ™€å -->\n",
        "    <div id=\"nudity-score\"></div>\n",
        "\n",
        "    <!-- ÿØ⁄©ŸÖŸá ÿ´ÿ®ÿ™ (ÿ®Ÿá ÿ¨ÿß€å ÿ¢ŸæŸÑŸàÿØ) -->\n",
        "    <div>\n",
        "      <button id=\"upload-btn\" class=\"btn\">ÿ´ÿ®ÿ™</button>\n",
        "      <button id=\"search-btn\" class=\"btn\">ÿ¨ÿ≥ÿ™ÿ¨Ÿà</button>\n",
        "    </div>\n",
        "\n",
        "    <!-- ŸÜŸÖÿß€åÿ¥ Ÿæ€åÿßŸÖ‚ÄåŸáÿß -->\n",
        "    <div id=\"message\"></div>\n",
        "\n",
        "    <!-- ŸÜŸÖÿß€åÿ¥ ŸÜÿ™ÿß€åÿ¨ ÿ¨ÿ≥ÿ™ÿ¨Ÿà (ÿ™ÿßŸÖÿ®ŸÜ€åŸÑ‚ÄåŸáÿß€å ÿ™ÿµÿßŸà€åÿ± ŸÖÿ¥ÿßÿ®Ÿá) -->\n",
        "    <div id=\"results\" class=\"thumbnails-container\"></div>\n",
        "  </div>\n",
        "\n",
        "  <!-- ŸÖÿØÿßŸÑ ÿ®ÿ±ÿß€å ŸÜŸÖÿß€åÿ¥ ÿ™ÿµŸà€åÿ± ⁄©ÿßŸÖŸÑ -->\n",
        "  <div id=\"myModal\" class=\"modal\">\n",
        "    <span class=\"close\">&times;</span>\n",
        "    <img class=\"modal-content\" id=\"modal-img\">\n",
        "  </div>\n",
        "\n",
        "  <script>\n",
        "    // ÿßŸÜÿ™ÿÆÿßÿ® ÿπŸÜÿßÿµÿ± ÿµŸÅÿ≠Ÿá\n",
        "    const dropZone = document.getElementById('drop-zone');\n",
        "    const fileInput = document.getElementById('file-input');\n",
        "    const preview = document.getElementById('preview');\n",
        "    const uploadBtn = document.getElementById('upload-btn');\n",
        "    const searchBtn = document.getElementById('search-btn');\n",
        "    const messageDiv = document.getElementById('message');\n",
        "    const resultsDiv = document.getElementById('results');\n",
        "    const nudityScoreDiv = document.getElementById('nudity-score');\n",
        "\n",
        "    let selectedFile = null;\n",
        "\n",
        "    // ⁄©ŸÑ€å⁄© ÿ±Ÿà€å ŸÜÿßÿ≠€åŸá ÿØÿ±⁄Ø‚ÄåÿßŸÜÿØ-ÿØÿ±ÿßŸæ ŸÅÿß€åŸÑ Ÿàÿ±ŸàÿØ€å ÿ±ÿß ŸÅÿπÿßŸÑ ŸÖ€å‚Äå⁄©ŸÜÿØ\n",
        "    dropZone.addEventListener('click', () => {\n",
        "      fileInput.click();\n",
        "    });\n",
        "\n",
        "    // ÿ≤ŸÖÿßŸÜ€å ⁄©Ÿá ⁄©ÿßÿ±ÿ®ÿ± ŸÅÿß€åŸÑ€å ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ÿ±ÿØÿå Ÿæ€åÿ¥‚ÄåŸÜŸÖÿß€åÿ¥ ÿ¢ŸÜ ŸÜŸÖÿß€åÿ¥ ÿØÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ\n",
        "    fileInput.addEventListener('change', (e) => {\n",
        "      if(e.target.files.length) {\n",
        "        selectedFile = e.target.files[0];\n",
        "        displayPreview(selectedFile);\n",
        "        computeNudity(selectedFile);\n",
        "      }\n",
        "    });\n",
        "\n",
        "    // ÿ±Ÿà€åÿØÿßÿØŸáÿß€å ÿØÿ±⁄Ø‚ÄåÿßŸÜÿØ-ÿØÿ±ÿßŸæ\n",
        "    dropZone.addEventListener('dragover', (e) => {\n",
        "      e.preventDefault();\n",
        "      dropZone.classList.add('hover');\n",
        "    });\n",
        "\n",
        "    dropZone.addEventListener('dragleave', (e) => {\n",
        "      e.preventDefault();\n",
        "      dropZone.classList.remove('hover');\n",
        "    });\n",
        "\n",
        "    dropZone.addEventListener('drop', (e) => {\n",
        "      e.preventDefault();\n",
        "      dropZone.classList.remove('hover');\n",
        "      if(e.dataTransfer.files.length) {\n",
        "        selectedFile = e.dataTransfer.files[0];\n",
        "        fileInput.files = e.dataTransfer.files; // ŸáŸÖ⁄ØÿßŸÖ‚Äåÿ≥ÿßÿ≤€å Ÿàÿ±ŸàÿØ€å ŸÅÿß€åŸÑ\n",
        "        displayPreview(selectedFile);\n",
        "        computeNudity(selectedFile);\n",
        "      }\n",
        "    });\n",
        "\n",
        "    // ÿ™ÿßÿ®ÿπ ŸÜŸÖÿß€åÿ¥ Ÿæ€åÿ¥‚ÄåŸÜŸÖÿß€åÿ¥ ÿ™ÿµŸà€åÿ±\n",
        "    function displayPreview(file) {\n",
        "      const reader = new FileReader();\n",
        "      reader.onload = function(e) {\n",
        "        preview.src = e.target.result;\n",
        "        preview.style.display = 'block';\n",
        "      }\n",
        "      reader.readAsDataURL(file);\n",
        "    }\n",
        "\n",
        "    // ÿ™ÿßÿ®ÿπ ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÜŸàÿØ€åÿ™€å ÿßÿ≤ ÿ∑ÿ±€åŸÇ endpoint ÿ¨ÿØ€åÿØ\n",
        "    function computeNudity(file) {\n",
        "      const formData = new FormData();\n",
        "      formData.append('file', file);\n",
        "\n",
        "      fetch('/calculate_nudity', {\n",
        "        method: 'POST',\n",
        "        body: formData\n",
        "      })\n",
        "      .then(response => response.json())\n",
        "      .then(data => {\n",
        "        if(data.error) {\n",
        "          nudityScoreDiv.innerHTML = `<p style=\"color:red;\">${data.error}</p>`;\n",
        "        } else {\n",
        "          // ŸÜŸÖÿß€åÿ¥ ŸÜŸàÿØ€åÿ™€å ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿ≤€åÿ®ÿß (ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿßÿ≥ÿ™ÿß€åŸÑ ÿØŸÑÿÆŸàÿßŸá ÿ±ÿß ÿßÿ∂ÿßŸÅŸá ⁄©ŸÜ€åÿØ)\n",
        "          const score = data.nudity_score;\n",
        "          nudityScoreDiv.innerHTML = `<p>ŸÜŸàÿØ€åÿ™€å: sexy = ${score.sexy} | porn = ${score.porn}</p>`;\n",
        "        }\n",
        "      })\n",
        "      .catch(err => {\n",
        "        nudityScoreDiv.innerHTML = `<p style=\"color:red;\">ÿÆÿ∑ÿß ÿØÿ± ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÜŸàÿØ€åÿ™€å.</p>`;\n",
        "      });\n",
        "    }\n",
        "\n",
        "    // ÿ±Ÿà€åÿØÿßÿØ ⁄©ŸÑ€å⁄© ÿØ⁄©ŸÖŸá ÿ´ÿ®ÿ™ (ÿ¢ŸæŸÑŸàÿØ)\n",
        "    uploadBtn.addEventListener('click', () => {\n",
        "      if(!selectedFile) {\n",
        "        alert(\"ŸÑÿ∑ŸÅÿßŸã ÿßÿ®ÿ™ÿØÿß €å⁄© ÿ™ÿµŸà€åÿ± ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ŸÜ€åÿØ.\");\n",
        "        return;\n",
        "      }\n",
        "      const formData = new FormData();\n",
        "      formData.append('file', selectedFile);\n",
        "\n",
        "      fetch('/', {\n",
        "        method: 'POST',\n",
        "        body: formData\n",
        "      })\n",
        "      .then(response => response.json())\n",
        "      .then(data => {\n",
        "        if(data.error) {\n",
        "          messageDiv.innerHTML = `<p style=\"color:red;\">${data.error}</p>`;\n",
        "        } else {\n",
        "          messageDiv.innerHTML = `<p style=\"color:green;\">ÿ´ÿ®ÿ™ ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿßŸÜÿ¨ÿßŸÖ ÿ¥ÿØ. ŸÜŸàÿØ€åÿ™€å: ${JSON.stringify(data.nudity_score)}</p>`;\n",
        "        }\n",
        "      })\n",
        "      .catch(err => {\n",
        "        messageDiv.innerHTML = `<p style=\"color:red;\">ÿÆÿ∑ÿß ÿØÿ± ÿ´ÿ®ÿ™ ÿ™ÿµŸà€åÿ±.</p>`;\n",
        "      });\n",
        "    });\n",
        "\n",
        "    // ÿ±Ÿà€åÿØÿßÿØ ⁄©ŸÑ€å⁄© ÿØ⁄©ŸÖŸá ÿ¨ÿ≥ÿ™ÿ¨Ÿà\n",
        "    searchBtn.addEventListener('click', () => {\n",
        "      if(!selectedFile) {\n",
        "        alert(\"ŸÑÿ∑ŸÅÿßŸã ÿßÿ®ÿ™ÿØÿß €å⁄© ÿ™ÿµŸà€åÿ± ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ŸÜ€åÿØ.\");\n",
        "        return;\n",
        "      }\n",
        "      const formData = new FormData();\n",
        "      formData.append('file', selectedFile);\n",
        "\n",
        "      fetch('/search', {\n",
        "        method: 'POST',\n",
        "        body: formData\n",
        "      })\n",
        "      .then(response => response.json())\n",
        "      .then(data => {\n",
        "        resultsDiv.innerHTML = ''; // Ÿæÿß⁄©ÿ≥ÿßÿ≤€å ŸÜÿ™ÿß€åÿ¨ ŸÇÿ®ŸÑ€å\n",
        "        if(data.error) {\n",
        "          messageDiv.innerHTML = `<p style=\"color:red;\">${data.error}</p>`;\n",
        "        } else {\n",
        "          // ÿß⁄Øÿ± ŸÜÿ™ÿß€åÿ¨ ŸÖÿ¥ÿßÿ®Ÿá Ÿàÿ¨ŸàÿØ ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ÿØ\n",
        "          if(data.similar_files) {\n",
        "            data.similar_files.forEach(item => {\n",
        "              const container = document.createElement('div');\n",
        "              container.className = 'thumb-container';\n",
        "\n",
        "              const img = document.createElement('img');\n",
        "              img.src = `/static/uploads/${item.filename}`;\n",
        "              img.alt = item.filename;\n",
        "              img.classList.add('thumbnail');\n",
        "              img.addEventListener('click', () => {\n",
        "                openModal(img.src);\n",
        "              });\n",
        "\n",
        "              const caption = document.createElement('div');\n",
        "              caption.textContent = `ÿ¥ÿ®ÿßŸáÿ™: ${item.similarity}%`;\n",
        "\n",
        "              container.appendChild(img);\n",
        "              container.appendChild(caption);\n",
        "              resultsDiv.appendChild(container);\n",
        "            });\n",
        "          } else if(data.best_match) {\n",
        "            const container = document.createElement('div');\n",
        "            container.className = 'thumb-container';\n",
        "\n",
        "            const img = document.createElement('img');\n",
        "            img.src = `/static/uploads/${data.best_match.filename}`;\n",
        "            img.alt = data.best_match.filename;\n",
        "            img.classList.add('thumbnail');\n",
        "            img.addEventListener('click', () => {\n",
        "              openModal(img.src);\n",
        "            });\n",
        "\n",
        "            const caption = document.createElement('div');\n",
        "            caption.textContent = `ÿ¥ÿ®ÿßŸáÿ™: ${data.best_match.similarity}%`;\n",
        "\n",
        "            container.appendChild(img);\n",
        "            container.appendChild(caption);\n",
        "            resultsDiv.appendChild(container);\n",
        "          }\n",
        "        }\n",
        "      })\n",
        "      .catch(err => {\n",
        "        messageDiv.innerHTML = `<p style=\"color:red;\">ÿÆÿ∑ÿß ÿØÿ± ÿ¨ÿ≥ÿ™ÿ¨Ÿà.</p>`;\n",
        "      });\n",
        "    });\n",
        "\n",
        "    // ŸÖÿØÿßŸÑ ŸÜŸÖÿß€åÿ¥ ÿ™ÿµŸà€åÿ± ⁄©ÿßŸÖŸÑ\n",
        "    const modal = document.getElementById(\"myModal\");\n",
        "    const modalImg = document.getElementById(\"modal-img\");\n",
        "    const closeModal = document.getElementsByClassName(\"close\")[0];\n",
        "\n",
        "    function openModal(src) {\n",
        "      modal.style.display = \"block\";\n",
        "      modalImg.src = src;\n",
        "    }\n",
        "\n",
        "    closeModal.onclick = function() {\n",
        "      modal.style.display = \"none\";\n",
        "    }\n",
        "\n",
        "    window.onclick = function(event) {\n",
        "      if (event.target == modal) {\n",
        "        modal.style.display = \"none\";\n",
        "      }\n",
        "    }\n",
        "  </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def upload_image():\n",
        "    if request.method == 'POST':\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({\"error\": \"ŸÅÿß€åŸÑ ÿßÿ±ÿ≥ÿßŸÑ ŸÜÿ¥ÿØŸá ÿßÿ≥ÿ™.\"})\n",
        "        file = request.files['file']\n",
        "        if file.filename == '' or not allowed_file(file.filename):\n",
        "            return jsonify({\"error\": \"ŸÜŸàÿπ ŸÅÿß€åŸÑ ŸÖÿπÿ™ÿ®ÿ± ŸÜ€åÿ≥ÿ™.\"})\n",
        "        file_path = os.path.join(UPLOAD_FOLDER, file.filename)\n",
        "        file.save(file_path)\n",
        "        nudity_score = detect_nudity(file_path)\n",
        "        embedding = generate_image_embedding(file_path)\n",
        "        if embedding is None:\n",
        "            return jsonify({\"error\": \"‚ùå ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ embedding ŸÜÿßŸÖŸàŸÅŸÇ ÿ®ŸàÿØ!\"})\n",
        "        duplicate_result = check_duplicate_image(file.filename, embedding, sim_threshold=0.9)\n",
        "        if duplicate_result[\"duplicate\"]:\n",
        "            if duplicate_result[\"reason\"] == \"filename\":\n",
        "                return jsonify({\"error\": duplicate_result[\"message\"]})\n",
        "            elif duplicate_result[\"reason\"] == \"embedding\":\n",
        "                return jsonify({\"error\": duplicate_result[\"message\"], \"similar_files\": duplicate_result[\"similar_files\"]})\n",
        "        save_to_database(file.filename, file_path, embedding, nudity_score)\n",
        "        index.add(embedding)\n",
        "        faiss.write_index(index, index_path)\n",
        "        return jsonify({\"nudity_score\": nudity_score})\n",
        "    # ŸÖÿ™ÿØ GET ÿµŸÅÿ≠Ÿá HTML ÿßÿµŸÑ€å ÿ±ÿß ŸÜŸÖÿß€åÿ¥ ŸÖ€å‚ÄåÿØŸáÿØ.\n",
        "    return render_template_string(html_page)\n",
        "\n",
        "@app.route('/search', methods=['POST'])\n",
        "def search_image():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"ŸÅÿß€åŸÑ ÿßÿ±ÿ≥ÿßŸÑ ŸÜÿ¥ÿØŸá ÿßÿ≥ÿ™.\"})\n",
        "    file = request.files['file']\n",
        "    if file.filename == '' or not allowed_file(file.filename):\n",
        "        return jsonify({\"error\": \"ŸÜŸàÿπ ŸÅÿß€åŸÑ ŸÖÿπÿ™ÿ®ÿ± ŸÜ€åÿ≥ÿ™.\"})\n",
        "    file_path = os.path.join(UPLOAD_FOLDER, file.filename)\n",
        "    file.save(file_path)\n",
        "    query_embedding = generate_image_embedding(file_path)\n",
        "    if query_embedding is None:\n",
        "         return jsonify({\"error\": \"ÿÆÿ∑ÿß ÿØÿ± ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß.\"})\n",
        "    embeddings, filenames = load_embeddings_from_db()\n",
        "    if embeddings.shape[0] == 0:\n",
        "         return jsonify({\"error\": \"Ÿá€å⁄Ü ÿ™ÿµŸà€åÿ±€å ÿØÿ± Ÿæÿß€å⁄ØÿßŸá ÿØÿßÿØŸá ŸÖŸàÿ¨ŸàÿØ ŸÜ€åÿ≥ÿ™.\"})\n",
        "    #// ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ÿ¥ÿ®ÿßŸáÿ™ ⁄©ÿ≥€åŸÜŸàÿ≥€å ÿ®ÿ±ÿß€å ÿ™ŸÖÿßŸÖ€å ÿ™ÿµÿßŸà€åÿ± ŸÖŸàÿ¨ŸàÿØ\n",
        "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1\n",
        "    normalized_embeddings = embeddings / norms\n",
        "    cosine_similarities = np.dot(normalized_query, normalized_embeddings.T)[0]\n",
        "    similar_indices = np.where(cosine_similarities >= 0.6)[0]  # ÿ¢ÿ≥ÿ™ÿßŸÜŸá 60%\n",
        "\n",
        "    similar_results = []\n",
        "    for idx in similar_indices:\n",
        "         similar_results.append({\n",
        "             \"filename\": filenames[idx],\n",
        "             \"similarity\": round(cosine_similarities[idx] * 100, 2)\n",
        "         })\n",
        "    if similar_results:\n",
        "         return jsonify({\"similar_files\": similar_results})\n",
        "    else:\n",
        "         index_temp = faiss.IndexFlatL2(D)\n",
        "         index_temp.add(embeddings)\n",
        "         _, I_arr = index_temp.search(query_embedding, 1)\n",
        "         best_index = I_arr[0][0]\n",
        "         best_filename = filenames[best_index]\n",
        "         best_similarity = round(cosine_similarities[best_index] * 100, 2)\n",
        "         return jsonify({\"best_match\": {\"filename\": best_filename, \"similarity\": best_similarity}})\n",
        "\n",
        "def is_similar_embedding(embedding, threshold=0.9):\n",
        "    embeddings_db, filenames = load_embeddings_from_db()\n",
        "    if embeddings_db.shape[0] == 0:\n",
        "        return False\n",
        "    normalized_query = embedding / np.linalg.norm(embedding)\n",
        "    norms = np.linalg.norm(embeddings_db, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1\n",
        "    normalized_embeddings = embeddings_db / norms\n",
        "    cosine_sim = np.max(np.dot(normalized_query, normalized_embeddings.T))\n",
        "    logging.info(f\"üîç ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿ¥ÿ®ÿßŸáÿ™ (cosine) €åÿßŸÅÿ™‚Äåÿ¥ÿØŸá: {cosine_sim * 100:.2f}%\")\n",
        "    return cosine_sim >= threshold\n",
        "\n",
        "import datetime\n",
        "import logging\n",
        "from PIL import Image\n",
        "\n",
        "def generate_image_fingerprint(image_path, username):\n",
        "    \"\"\"\n",
        "    ÿ™ŸàŸÑ€åÿØ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ™ÿµŸà€åÿ± ÿ®ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ÿßŸÑ⁄ØŸàÿ±€åÿ™ŸÖ dHash Ÿà ÿßÿØÿ∫ÿßŸÖ ÿ®ÿß €åŸàÿ≤ÿ±ŸÜ€åŸÖ (ÿ®Ÿá ÿµŸàÿ±ÿ™ Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑ) Ÿà ÿ≤ŸÖÿßŸÜ ŸÅÿ±ÿßÿÆŸàÿßŸÜ€å.\n",
        "\n",
        "    Ÿàÿ±ŸàÿØ€å‚ÄåŸáÿß:\n",
        "      image_path: ŸÖÿ≥€åÿ± ŸÅÿß€åŸÑ ÿ™ÿµŸà€åÿ±.\n",
        "      username: ŸÜÿßŸÖ ⁄©ÿßÿ±ÿ®ÿ±€å (ÿµÿßÿ≠ÿ® ŸÖÿ≠ÿ™Ÿàÿß).\n",
        "\n",
        "    ÿÆÿ±Ÿàÿ¨€å:\n",
        "      ÿ±ÿ¥ÿ™Ÿá‚Äåÿß€å ÿ¥ÿßŸÖŸÑ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ™ÿµŸà€åÿ± ÿ®Ÿá ÿµŸàÿ±ÿ™ Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑÿå ŸÜÿßŸÖ ⁄©ÿßÿ±ÿ®ÿ±€å ÿ®Ÿá ÿµŸàÿ±ÿ™ Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑÿå\n",
        "      Ÿà ÿ™ÿßÿ±€åÿÆ/ÿ≥ÿßÿπÿ™ (UTC) ŸÅÿ±ÿßÿÆŸàÿßŸÜ€å ÿ™ÿßÿ®ÿπ ÿ®Ÿá ÿµŸàÿ±ÿ™ Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑ.\n",
        "      (ŸÖÿ´ŸÑÿßŸã: \"f1a2b3c4d5e6f789-756365726e616d-323032353032313031323335393937383132\")\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ÿØÿ±€åÿßŸÅÿ™ ÿ≤ŸÖÿßŸÜ ŸÅÿ±ÿßÿÆŸàÿßŸÜ€å ÿ™ÿßÿ®ÿπ (ÿ®Ÿá ÿµŸàÿ±ÿ™ UTC ÿ®ÿß ÿØŸÇÿ™ ŸÖ€å⁄©ÿ±Ÿàÿ´ÿßŸÜ€åŸá)\n",
        "        now_str = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "        now_hex = now_str.encode('utf-8').hex()\n",
        "\n",
        "        # ÿ®ÿßÿ≤ ⁄©ÿ±ÿØŸÜ ÿ™ÿµŸà€åÿ± Ÿà ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá grayscale\n",
        "        image = Image.open(image_path).convert(\"L\")\n",
        "        # ÿ™ÿ∫€å€åÿ± ÿßŸÜÿØÿßÿ≤Ÿá ÿ™ÿµŸà€åÿ± ÿ®Ÿá 9x8 (ÿ®ÿ±ÿß€å ÿß€åÿ¨ÿßÿØ 64 ÿ®€åÿ™)\n",
        "        image = image.resize((9, 8), Image.ANTIALIAS)\n",
        "        # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÖŸÇÿßÿØ€åÿ± Ÿæ€å⁄©ÿ≥ŸÑ€å\n",
        "        pixels = list(image.getdata())\n",
        "        # ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ÿ™ŸÅÿßŸàÿ™ ÿ®€åŸÜ Ÿæ€å⁄©ÿ≥ŸÑ‚ÄåŸáÿß€å ŸÖÿ¨ÿßŸàÿ± (ÿßŸÑ⁄ØŸàÿ±€åÿ™ŸÖ dHash)\n",
        "        difference = []\n",
        "        for row in range(8):\n",
        "            for col in range(8):\n",
        "                left_pixel = pixels[row * 9 + col]\n",
        "                right_pixel = pixels[row * 9 + col + 1]\n",
        "                difference.append(1 if left_pixel > right_pixel else 0)\n",
        "        # ÿ™ÿ®ÿØ€åŸÑ ŸÑ€åÿ≥ÿ™ ÿ®€åÿ™€å ÿ®Ÿá ÿπÿØÿØ 64 ÿ®€åÿ™€å\n",
        "        hash_val = 0\n",
        "        for bit in difference:\n",
        "            hash_val = (hash_val << 1) | bit\n",
        "        # ÿ™ÿ®ÿØ€åŸÑ ÿπÿØÿØ ÿ®Ÿá ÿ±ÿ¥ÿ™Ÿá Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑ 16 ÿ±ŸÇŸÖ€å\n",
        "        image_fingerprint = hex(hash_val)[2:].rjust(16, '0')\n",
        "\n",
        "        # ÿ™ÿ®ÿØ€åŸÑ €åŸàÿ≤ÿ±ŸÜ€åŸÖ ÿ®Ÿá Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑ\n",
        "        username_hex = username.encode('utf-8').hex()\n",
        "\n",
        "        # ÿßÿØÿ∫ÿßŸÖ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ™ÿµŸà€åÿ±ÿå €åŸàÿ≤ÿ±ŸÜ€åŸÖ Ÿá⁄Øÿ≤ÿß ÿ¥ÿØŸá Ÿà ÿ≤ŸÖÿßŸÜ (Ÿá⁄Øÿ≤ÿß) ÿ®ÿß ÿ¨ÿØÿß⁄©ŸÜŸÜÿØŸá \"-\"\n",
        "        combined_fingerprint = image_fingerprint + \"-\" + username_hex + \"-\" + now_hex\n",
        "\n",
        "        return combined_fingerprint\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿ™ŸàŸÑ€åÿØ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def embed_fingerprint_in_image(image_path, output_path, fingerprint):\n",
        "    \"\"\"\n",
        "    ÿØÿ±ÿ¨ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿ™ÿßÿØ€åÿ™ÿß ÿØÿ± ÿ™ÿµŸà€åÿ±.\n",
        "\n",
        "    Ÿæÿßÿ±ÿßŸÖÿ™ÿ±Ÿáÿß:\n",
        "      image_path: ŸÖÿ≥€åÿ± ÿ™ÿµŸà€åÿ± Ÿàÿ±ŸàÿØ€å.\n",
        "      output_path: ŸÖÿ≥€åÿ± ÿ∞ÿÆ€åÿ±Ÿá ÿ™ÿµŸà€åÿ± ÿ¨ÿØ€åÿØ ÿ®ÿß ŸÖÿ™ÿßÿØ€åÿ™ÿß€å ÿ®Ÿá‚Äåÿ±Ÿàÿ≤ ÿ¥ÿØŸá.\n",
        "      fingerprint: ÿ±ÿ¥ÿ™Ÿá ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ (ŸÖÿ´ŸÑÿßŸã ÿ≠ÿßÿµŸÑ ÿßÿ≤ ÿ™ÿßÿ®ÿπ generate_image_fingerprint).\n",
        "\n",
        "    ÿß€åŸÜ ÿ™ÿßÿ®ÿπ ÿ®ÿ≥ÿ™Ÿá ÿ®Ÿá ŸÅÿ±ŸÖÿ™ ÿ™ÿµŸà€åÿ± (jpeg, jpg, png, bmp, webp) ÿßÿ≤ ÿ±Ÿàÿ¥ ŸÖŸÜÿßÿ≥ÿ® ÿ®ÿ±ÿß€å ÿØÿ±ÿ¨ ŸÖÿ™ÿßÿØ€åÿ™ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜÿØ.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ÿ®ÿßÿ≤ ⁄©ÿ±ÿØŸÜ ÿ™ÿµŸà€åÿ± ÿ®ÿß PIL\n",
        "        image = Image.open(image_path)\n",
        "        # ⁄Øÿ±ŸÅÿ™ŸÜ Ÿæÿ≥ŸàŸÜÿØ ŸÅÿß€åŸÑ ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿ≠ÿ±ŸàŸÅ ⁄©Ÿà⁄Ü⁄©\n",
        "        ext = os.path.splitext(image_path)[1].lower()\n",
        "\n",
        "        if ext in [\".jpeg\", \".jpg\"]:\n",
        "            # ÿ®ÿ±ÿß€å JPEG/JPG: ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ⁄©ÿ™ÿßÿ®ÿÆÿßŸÜŸá piexif ÿ®ÿ±ÿß€å ÿØÿ±ÿ¨ ÿØÿ± ŸÅ€åŸÑÿØ UserComment\n",
        "            if \"exif\" in image.info:\n",
        "                exif_dict = piexif.load(image.info[\"exif\"])\n",
        "            else:\n",
        "                exif_dict = {\"0th\": {}, \"Exif\": {}, \"GPS\": {}, \"1st\": {}, \"thumbnail\": None}\n",
        "            # ÿØÿ±ÿ¨ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿØÿ± ŸÅ€åŸÑÿØ UserComment ÿØÿ± ÿ®ÿÆÿ¥ ExifIFD\n",
        "            exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = fingerprint.encode(\"utf-8\")\n",
        "            exif_bytes = piexif.dump(exif_dict)\n",
        "            image.save(output_path, \"jpeg\", exif=exif_bytes)\n",
        "            logging.info(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿ™ÿßÿØ€åÿ™ÿß ÿØÿ± ŸÅÿß€åŸÑ JPEG/JPG ÿØÿ±ÿ¨ ÿ¥ÿØ.\")\n",
        "\n",
        "        elif ext == \".png\":\n",
        "            # ÿ®ÿ±ÿß€å PNG: ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ PngInfo ÿ¨Ÿáÿ™ ÿßŸÅÿ≤ŸàÿØŸÜ ÿ™⁄Ø ŸÖÿ™ŸÜ€å\n",
        "            png_info = PngImagePlugin.PngInfo()\n",
        "            png_info.add_text(\"Fingerprint\", fingerprint)\n",
        "            image.save(output_path, \"png\", pnginfo=png_info)\n",
        "            logging.info(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿ™ÿßÿØ€åÿ™ÿß ÿØÿ± ŸÅÿß€åŸÑ PNG ÿØÿ±ÿ¨ ÿ¥ÿØ.\")\n",
        "\n",
        "        elif ext == \".bmp\":\n",
        "            # ÿ®ÿ±ÿß€å BMP: ŸÅÿ±ŸÖÿ™ BMP ÿßÿ≥ÿ™ÿßŸÜÿØÿßÿ±ÿØ Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ⁄©ÿßŸÖŸÑ€å ÿßÿ≤ ŸÖÿ™ÿßÿØ€åÿ™ÿß ŸÜÿØÿßÿ±ÿØ.\n",
        "            # ÿ™ŸÑÿßÿ¥ ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ ÿ™ÿß €å⁄© ⁄©ŸÑ€åÿØ comment ÿ®Ÿá info ÿßÿ∂ÿßŸÅŸá ÿ¥ŸàÿØ.\n",
        "            if not hasattr(image, \"info\"):\n",
        "                image.info = {}\n",
        "            image.info[\"comment\"] = fingerprint  # ŸÖŸÖ⁄©ŸÜ ÿßÿ≥ÿ™ ÿ™Ÿàÿ≥ÿ∑ ÿ®ÿ±ÿÆ€å ÿ®ÿ±ŸÜÿßŸÖŸá‚ÄåŸáÿß ÿÆŸàÿßŸÜÿØŸá ÿ¥ŸàÿØ.\n",
        "            image.save(output_path, \"bmp\")\n",
        "            logging.info(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ (ÿ®Ÿá ÿµŸàÿ±ÿ™ comment) ÿØÿ± ŸÅÿß€åŸÑ BMP ÿØÿ±ÿ¨ ÿ¥ÿØ (ÿ™Ÿàÿ¨Ÿá: Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ŸÖÿ≠ÿØŸàÿØ).\")\n",
        "\n",
        "        elif ext == \".webp\":\n",
        "            # ÿ®ÿ±ÿß€å WEBP: ÿß⁄Øÿ±⁄ÜŸá Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ÿßÿ≤ ŸÖÿ™ÿßÿØ€åÿ™ÿß ÿØÿ± ÿß€åŸÜ ŸÅÿ±ŸÖÿ™ ŸÖÿ≠ÿØŸàÿØ ÿßÿ≥ÿ™ÿå\n",
        "            # ŸÖ€å‚Äåÿ™ŸàÿßŸÜ ÿßÿ≤ exif ÿ®ÿ±ÿß€å ÿØÿ±ÿ¨ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ÿ±ÿØ (ÿ™Ÿàÿ¨Ÿá: ÿ™ŸÜŸáÿß ÿ®ÿ±ÿß€å ÿ™ÿµÿßŸà€åÿ± lossless ⁄©ÿßÿ±ÿß€å€å ÿØÿßÿ±ÿØ).\n",
        "            if \"exif\" in image.info:\n",
        "                exif_dict = piexif.load(image.info[\"exif\"])\n",
        "            else:\n",
        "                exif_dict = {\"0th\": {}, \"Exif\": {}, \"GPS\": {}, \"1st\": {}, \"thumbnail\": None}\n",
        "            exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = fingerprint.encode(\"utf-8\")\n",
        "            exif_bytes = piexif.dump(exif_dict)\n",
        "            image.save(output_path, \"WEBP\", exif=exif_bytes)\n",
        "            logging.info(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿ™ÿßÿØ€åÿ™ÿß ÿØÿ± ŸÅÿß€åŸÑ WEBP ÿØÿ±ÿ¨ ÿ¥ÿØ.\")\n",
        "\n",
        "        else:\n",
        "            logging.error(\"ŸÅÿ±ŸÖÿ™ ÿ™ÿµŸà€åÿ± Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ŸÜŸÖ€å‚Äåÿ¥ŸàÿØ.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"ÿÆÿ∑ÿß ÿØÿ± ÿØÿ±ÿ¨ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿØÿ± ŸÖÿ™ÿßÿØ€åÿ™ÿß: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def parse_image_fingerprint(fingerprint):\n",
        "    \"\"\"\n",
        "    ÿ™ŸÅÿ≥€åÿ± ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ™ÿµŸà€åÿ± Ÿà ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ¢ŸÜ.\n",
        "\n",
        "    Ÿàÿ±ŸàÿØ€å:\n",
        "      fingerprint: ÿ±ÿ¥ÿ™Ÿá ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ™ŸàŸÑ€åÿØ ÿ¥ÿØŸá.\n",
        "\n",
        "    ÿÆÿ±Ÿàÿ¨€å:\n",
        "      ÿØ€å⁄©ÿ¥ŸÜÿ±€å ÿ¥ÿßŸÖŸÑ:\n",
        "        - image_fingerprint: ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿ™ÿµŸà€åÿ± (dHash Ÿá⁄Øÿ≤ÿßÿØÿ≥€åŸÖÿßŸÑ)\n",
        "        - username: ŸÜÿßŸÖ ⁄©ÿßÿ±ÿ®ÿ±€å ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ¥ÿØŸá ÿßÿ≤ ŸÖŸÇÿØÿßÿ± Ÿá⁄Øÿ≤\n",
        "        - timestamp: ÿ™ÿßÿ±€åÿÆ Ÿà ÿ≤ŸÖÿßŸÜ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ¥ÿØŸá ÿßÿ≤ ŸÖŸÇÿØÿßÿ± Ÿá⁄Øÿ≤\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ÿ®ÿ±ÿ±ÿ≥€å ÿ≥ÿßÿÆÿ™ÿßÿ± ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™\n",
        "        parts = fingerprint.split(\"-\")\n",
        "        if len(parts) != 3:\n",
        "            raise ValueError(\"ÿ≥ÿßÿÆÿ™ÿßÿ± ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ŸÜÿßŸÖÿπÿ™ÿ®ÿ± ÿßÿ≥ÿ™.\")\n",
        "\n",
        "        image_fingerprint, username_hex, timestamp_hex = parts\n",
        "\n",
        "        # ÿ™ÿ®ÿØ€åŸÑ Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑ €åŸàÿ≤ÿ±ŸÜ€åŸÖ ÿ®Ÿá ŸÖÿ™ŸÜ ÿßÿµŸÑ€å\n",
        "        username = binascii.unhexlify(username_hex).decode('utf-8')\n",
        "\n",
        "        # ÿ™ÿ®ÿØ€åŸÑ Ÿá⁄Øÿ≤ÿß ÿØÿ≥€åŸÖÿßŸÑ ÿ≤ŸÖÿßŸÜ ÿ®Ÿá ŸÖŸÇÿØÿßÿ± ÿÆŸàÿßŸÜÿß\n",
        "        timestamp_str = binascii.unhexlify(timestamp_hex).decode('utf-8')\n",
        "        timestamp = datetime.datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S%f\")\n",
        "\n",
        "        return {\n",
        "            \"image_fingerprint\": image_fingerprint,\n",
        "            \"username\": username,\n",
        "            \"timestamp\": timestamp\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿ™ŸÅÿ≥€åÿ± ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ŸÖÿ´ÿßŸÑ ÿßÿ≥ÿ™ŸÅÿßÿØŸá:\n",
        "# ŸÅÿ±ÿ∂ ⁄©ŸÜ€åÿØ fingerprint ÿßÿ≤ ÿ™ÿßÿ®ÿπ generate_image_fingerprint ÿ®Ÿá ÿØÿ≥ÿ™ ÿ¢ŸÖÿØŸá ÿ®ÿßÿ¥ÿØ.\n",
        "# fingerprint = generate_image_fingerprint(\"path/to/image.jpg\", \"myusername\")\n",
        "# embed_fingerprint_in_image(\"path/to/image.jpg\", \"path/to/output_image.jpg\", fingerprint)\n",
        "\n",
        "import os\n",
        "import piexif\n",
        "from PIL import Image, PngImagePlugin\n",
        "import logging\n",
        "\n",
        "def extract_fingerprint_from_image(image_path):\n",
        "    \"\"\"\n",
        "    ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿßÿ≤ ŸÖÿ™ÿßÿØ€åÿ™ÿß€å ÿ™ÿµŸà€åÿ±.\n",
        "\n",
        "    Ÿæÿßÿ±ÿßŸÖÿ™ÿ±Ÿáÿß:\n",
        "      image_path: ŸÖÿ≥€åÿ± ÿ™ÿµŸà€åÿ±.\n",
        "\n",
        "    ÿÆÿ±Ÿàÿ¨€å:\n",
        "      ŸÖŸÇÿØÿßÿ± ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿß⁄Øÿ± ŸÖŸàÿ¨ŸàÿØ ÿ®ÿßÿ¥ÿØÿå ÿØÿ± ÿ∫€åÿ± ÿß€åŸÜ ÿµŸàÿ±ÿ™ ŸÖŸÇÿØÿßÿ± None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ÿ®ÿßÿ≤ ⁄©ÿ±ÿØŸÜ ÿ™ÿµŸà€åÿ± ÿ®ÿß PIL\n",
        "        image = Image.open(image_path)\n",
        "        # ⁄Øÿ±ŸÅÿ™ŸÜ Ÿæÿ≥ŸàŸÜÿØ ŸÅÿß€åŸÑ ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿ≠ÿ±ŸàŸÅ ⁄©Ÿà⁄Ü⁄©\n",
        "        ext = os.path.splitext(image_path)[1].lower()\n",
        "\n",
        "        if ext in [\".jpeg\", \".jpg\"]:\n",
        "            # ÿ®ÿ±ÿß€å JPEG/JPG: ÿÆŸàÿßŸÜÿØŸÜ ÿßÿ≤ EXIF\n",
        "            if \"exif\" in image.info:\n",
        "                exif_dict = piexif.load(image.info[\"exif\"])\n",
        "                if piexif.ExifIFD.UserComment in exif_dict[\"Exif\"]:\n",
        "                    fingerprint_bytes = exif_dict[\"Exif\"][piexif.ExifIFD.UserComment]\n",
        "                    fingerprint = fingerprint_bytes.decode(\"utf-8\")  # ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá ÿ±ÿ¥ÿ™Ÿá\n",
        "                    return fingerprint\n",
        "            logging.warning(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿØÿ± ŸÖÿ™ÿßÿØ€åÿ™ÿß€å EXIF ŸÅÿß€åŸÑ JPEG/JPG €åÿßŸÅÿ™ ŸÜÿ¥ÿØ.\")\n",
        "\n",
        "        elif ext == \".png\":\n",
        "            # ÿ®ÿ±ÿß€å PNG: ÿÆŸàÿßŸÜÿØŸÜ ÿßÿ≤ PngInfo\n",
        "            png_info = image.info\n",
        "            if \"Fingerprint\" in png_info:\n",
        "                return png_info[\"Fingerprint\"]\n",
        "            logging.warning(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿØÿ± ŸÖÿ™ÿßÿØ€åÿ™ÿß€å PNG €åÿßŸÅÿ™ ŸÜÿ¥ÿØ.\")\n",
        "\n",
        "        elif ext == \".bmp\":\n",
        "            # ÿ®ÿ±ÿß€å BMP: ÿÆŸàÿßŸÜÿØŸÜ ÿßÿ≤ info\n",
        "            if \"comment\" in image.info:\n",
        "                return image.info[\"comment\"]\n",
        "            logging.warning(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿØÿ± ŸÖÿ™ÿßÿØ€åÿ™ÿß€å BMP €åÿßŸÅÿ™ ŸÜÿ¥ÿØ.\")\n",
        "\n",
        "        elif ext == \".webp\":\n",
        "            # ÿ®ÿ±ÿß€å WEBP: ÿÆŸàÿßŸÜÿØŸÜ ÿßÿ≤ EXIF\n",
        "            if \"exif\" in image.info:\n",
        "                exif_dict = piexif.load(image.info[\"exif\"])\n",
        "                if piexif.ExifIFD.UserComment in exif_dict[\"Exif\"]:\n",
        "                    fingerprint_bytes = exif_dict[\"Exif\"][piexif.ExifIFD.UserComment]\n",
        "                    fingerprint = fingerprint_bytes.decode(\"utf-8\")  # ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá ÿ±ÿ¥ÿ™Ÿá\n",
        "                    return fingerprint\n",
        "            logging.warning(\"ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿØÿ± ŸÖÿ™ÿßÿØ€åÿ™ÿß€å WEBP €åÿßŸÅÿ™ ŸÜÿ¥ÿØ.\")\n",
        "\n",
        "        else:\n",
        "            logging.error(\"ŸÅÿ±ŸÖÿ™ ÿ™ÿµŸà€åÿ± Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ŸÜŸÖ€å‚Äåÿ¥ŸàÿØ.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"ÿÆÿ∑ÿß ÿØÿ± ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÅ€åŸÜ⁄Øÿ±Ÿæÿ±€åŸÜÿ™ ÿßÿ≤ ŸÖÿ™ÿßÿØ€åÿ™ÿß: {e}\")\n",
        "        return None\n",
        "\n",
        "# ŸÖÿ´ÿßŸÑ ÿßÿ≥ÿ™ŸÅÿßÿØŸá:\n",
        "# fingerprint = extract_fingerprint_from_image(\"path/to/image.jpg\")\n",
        "# if fingerprint:\n",
        "#     parsed_data = parse_image_fingerprint(fingerprint)\n",
        "#     print(parsed_data)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"üåç ÿßÿ¨ÿ±ÿß ÿ¥ÿØŸá ÿØÿ±: {public_url}\")\n",
        "    app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install piexif\n"
      ],
      "metadata": {
        "id": "V7iE92SQE1X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU1f5YCwmQhu"
      },
      "outputs": [],
      "source": [
        "import flask\n",
        "import pyngrok\n",
        "import numpy\n",
        "import faiss\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import timm\n",
        "import tensorflow_hub\n",
        "\n",
        "print(\"Flask:\", flask.__version__)\n",
        "print(\"Pyngrok:\", pyngrok.__version__)\n",
        "print(\"NumPy:\", numpy.__version__)\n",
        "print(\"FAISS:\", faiss.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"Timm:\", timm.__version__)\n",
        "print(\"TensorFlow Hub:\", tensorflow_hub.__version__)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6I+fJzzLYrOKbwoiPjt3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}